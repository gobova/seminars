{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear and Quadratic Discriminant Analysis\n",
    "\n",
    "Assumptions:\n",
    "- Multivariate normality: Independent variables are normal for each level of the grouping variable.\n",
    "- Homogeneity of variance/covariance (homoscedasticity): Variances among group variables are the same across levels of predictors. Can be tested with Box's M statistic. It has been suggested, however, that linear discriminant analysis be used when covariances are equal, and that quadratic discriminant analysis may be used when covariances are not equal.\n",
    "- Multicollinearity: Predictive power can decrease with an increased correlation between predictor variables.\n",
    "- Independence: Participants are assumed to be randomly sampled, and a participant's score on one variable is assumed to be independent of scores on that variable for all other participants.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_lda_qda_001.png)\n",
    "\n",
    "\n",
    "## AdaBoost\n",
    "\n",
    "The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction. The data modifications at each so-called boosting iteration consist of applying weights to each of the training samples. Initially, those weights are all set to, so that the first step simply trains a weak learner on the original data.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*8VLJWDtYRehTEZXTLhEQog.png)\n",
    "\n",
    "\n",
    "## Bagging\n",
    "\n",
    "In ensemble algorithms, bagging methods form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction. These methods are used as a way to reduce the variance of a base estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it. In many cases, bagging methods constitute a very simple way to improve with respect to a single model, without making it necessary to adapt the underlying base algorithm. \n",
    "\n",
    "![](https://ars.els-cdn.com/content/image/1-s2.0-S1566253520303195-gr1.jpg)\n",
    "\n",
    "## Extra Trees Ensemble / Random Forest\n",
    "\n",
    "![](https://miro.medium.com/max/1200/1*hmtbIgxoflflJqMJ_UHwXw.jpeg)\n",
    "\n",
    "## Gradient Boosting\n",
    "\n",
    "![](https://media.geeksforgeeks.org/wp-content/uploads/20200721214745/gradientboosting.PNG)\n",
    "\n",
    "## Naive Bayes\n",
    "![](https://miro.medium.com/max/1200/0*qFuHAV7Vd09064q-.jpeg)\n",
    "\n",
    "\n",
    "## MultiLayer Perception\n",
    "\n",
    "![](https://michael-fuchs-python.netlify.app/post/2021-02-03-nn-multi-layer-perceptron-classifier-mlpclassifier_files/p110p3.png)\n",
    "\n",
    "## SVM\n",
    "![](https://www.researchgate.net/publication/332777280/figure/fig2/AS:753584035602432@1556679843419/A-linear-Support-Vector-Machine-SVM-case.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
