{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorical Encoding\n",
    "![](https://miro.medium.com/max/2400/1*FPz875Pa14_5wwedQ9irPA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://www.kaggle.com/subinium/11-categorical-encoders-and-benchmark/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Inspired by this article and the repo, I have created the following kernel:\n",
    "\n",
    "- [Benchmarking Categorical Encoders](https://towardsdatascience.com/benchmarking-categorical-encoders-9c322bd77ee8)\n",
    "\n",
    "- [CategoricalEncodingBenchmark](https://github.com/DenisVorotyntsev/CategoricalEncodingBenchmark)\n",
    "\n",
    "Let's see how these methods work in this dataset.\n",
    "\n",
    "[Discussion](https://www.kaggle.com/c/cat-in-the-dat/discussion/112584)\n",
    "\n",
    "- no feature preprocessing\n",
    "- Use KFold(5) for CV (+ more fold get better score)\n",
    "- LR (C=0.1, solver=lbfgs)\n",
    "\n",
    "|Encoder|LB Score|\n",
    "|-|-|\n",
    "|TE|0.78018|\n",
    "|WOE|0.78861|\n",
    "|LOOE|0.79382|\n",
    "|James-Stein|0.77843|\n",
    "|Catboost|0.79164|\n",
    "|One-Hot(another my kernel)|0.77973|\n",
    "\n",
    "\n",
    "\n",
    "### Category-Encoders\n",
    "\n",
    "1. Label Encoder\n",
    "2. One-Hot Encoder\n",
    "3. Sum Encoder\n",
    "4. Helmert Encoder\n",
    "5. Frequency Encoder\n",
    "6. Target Encoder\n",
    "7. M-Estimate Encoder\n",
    "8. Weight Of Evidence Encoder\n",
    "9. James-Stein Encoder\n",
    "10. Leave-one-out Encoder\n",
    "11. Catboost Encoder\n",
    "---\n",
    "- Validation (Benchmark)\n",
    "    - single LR\n",
    "    - LR with Cross Validation\n",
    "\n",
    "- Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Category-Encoders \n",
    "\n",
    "A set of scikit-learn-style transformers for encoding categorical variables into numeric by means of different techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category-encoders\n",
      "  Downloading category_encoders-2.3.0-py2.py3-none-any.whl (82 kB)\n",
      "     |████████████████████████████████| 82 kB 162 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: pandas>=0.21.1 in /home/@share/Documents/hse/seminars/venv/lib/python3.9/site-packages (from category-encoders) (1.3.3)\n",
      "Collecting patsy>=0.5.1\n",
      "  Downloading patsy-0.5.2-py2.py3-none-any.whl (233 kB)\n",
      "     |████████████████████████████████| 233 kB 1.6 MB/s            \n",
      "\u001b[?25hCollecting statsmodels>=0.9.0\n",
      "  Downloading statsmodels-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
      "     |████████████████████████████████| 9.9 MB 899 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /home/@share/Documents/hse/seminars/venv/lib/python3.9/site-packages (from category-encoders) (1.0)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /home/@share/Documents/hse/seminars/venv/lib/python3.9/site-packages (from category-encoders) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /home/@share/Documents/hse/seminars/venv/lib/python3.9/site-packages (from category-encoders) (1.21.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/@share/Documents/hse/seminars/venv/lib/python3.9/site-packages (from pandas>=0.21.1->category-encoders) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/@share/Documents/hse/seminars/venv/lib/python3.9/site-packages (from pandas>=0.21.1->category-encoders) (2021.1)\n",
      "Requirement already satisfied: six in /home/@share/Documents/hse/seminars/venv/lib/python3.9/site-packages (from patsy>=0.5.1->category-encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/@share/Documents/hse/seminars/venv/lib/python3.9/site-packages (from scikit-learn>=0.20.0->category-encoders) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/@share/Documents/hse/seminars/venv/lib/python3.9/site-packages (from scikit-learn>=0.20.0->category-encoders) (3.0.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/@share/Documents/hse/seminars/venv/lib/python3.9/site-packages (from statsmodels>=0.9.0->category-encoders) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/@share/Documents/hse/seminars/venv/lib/python3.9/site-packages (from packaging>=21.3->statsmodels>=0.9.0->category-encoders) (2.4.7)\n",
      "Installing collected packages: patsy, statsmodels, category-encoders\n",
      "Successfully installed category-encoders-2.3.0 patsy-0.5.2 statsmodels-0.13.2\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/home/@share/Documents/hse/seminars/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# If you want to test this on your local notebook\n",
    "# http://contrib.scikit-learn.org/categorical-encoding/\n",
    "!pip install category-encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from category_encoders.woe import WOEEncoder\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from category_encoders.sum_coding import SumEncoder\n",
    "from category_encoders.m_estimate import MEstimateEncoder\n",
    "from category_encoders.leave_one_out import LeaveOneOutEncoder\n",
    "from category_encoders.helmert import HelmertEncoder\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "from category_encoders.james_stein import JamesSteinEncoder\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "\n",
    "TEST = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read csv and doing some preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.06 s, sys: 292 ms, total: 2.36 s\n",
      "Wall time: 2.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train = pd.read_csv('cat-in-the-dat/train.csv')\n",
    "test = pd.read_csv('cat-in-the-dat/test.csv')\n",
    "target = train['target']\n",
    "train_id = train['id']\n",
    "test_id = test['id']\n",
    "train.drop(['target', 'id'], axis=1, inplace=True)\n",
    "test.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = list(train.columns) # you can custumize later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### notation\n",
    "\n",
    "- $y$ and $y+$ — the total number of observations and the total number of positive observations (y=1);\n",
    "- $x_i$, $y_i$ — the i-th value of category and target;\n",
    "- $n$ and $n+$ — the number of observations and the number of positive observations (y=1) for a given value of a categorical column;\n",
    "- $a$ — a regularization hyperparameter (selected by a user), prior — an average value of the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Label Encoder (LE), Ordinary Encoder(OE)\n",
    "\n",
    "One of the most common encoding methods.\n",
    "\n",
    "An encoding method that converts categorical data into numbers.\n",
    "The code is very simple, and when you encode a specific column you can proceed as follows:\n",
    "\n",
    "``` python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label = LabelEncoder()\n",
    "\n",
    "train[column_name] = label.fit_transform(train[column_name])\n",
    "```\n",
    "\n",
    "The simple idea is to convert the same category to a number with the same value.\n",
    "\n",
    "So the range of numbers maps from 0 to n-1 as labels.\n",
    "\n",
    "The disadvantage is that the labels are ordered randomly (in the existing order of the data), which can add noise while assigning an unexpected order between labels. In other words, the data becomes ordinary (ordinal, ordered) data, which can lead to unintended consequences.\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.07 s, sys: 817 ms, total: 6.88 s\n",
      "Wall time: 5.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "LE_encoder = OrdinalEncoder(feature_list)\n",
    "train_le = LE_encoder.fit_transform(train)\n",
    "test_le = LE_encoder.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. One-Hot Encoder (OHE, dummy encoder)\n",
    "\n",
    "\n",
    "So what can you do to give values ​​by category instead of ordering them?\n",
    "\n",
    "If you have data with specific category values, you can create a column. If the base Label Encoder label type is N, then OHE is the way to create N columns.\n",
    "\n",
    "Since only the row containing the content is given as 1, it is called one-hot encoding. Also called dummy encoding in the sense of creating a dummy.\n",
    "\n",
    "\n",
    "In this competition:\n",
    "\n",
    "``` python\n",
    "traintest = pd.concat([train, test])\n",
    "dummies = pd.get_dummies(traintest, columns=traintest.columns, drop_first=True, sparse=True)\n",
    "train_ohe = dummies.iloc[:train.shape[0], :]\n",
    "test_ohe = dummies.iloc[train.shape[0]:, :]\n",
    "train_ohe = train_ohe.sparse.to_coo().tocsr()\n",
    "test_ohe = test_ohe.sparse.to_coo().tocsr()\n",
    "```\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# this method didn't work because of RAM memory. \n",
    "# so we have to use pd.dummies \n",
    "# OHE_encoder = OneHotEncoder(feature_list)\n",
    "# train_ohe = OHE_encoder.fit_transform(train)\n",
    "# test_ohe = OHE_encoder.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sum Encoder (Deviation Encoder, Effect Encoder)\n",
    "\n",
    "**Sum Encoder** compares the mean of the dependent variable (target) for a given level of a categorical column to the overall mean of the target. \n",
    "\n",
    "Sum Encoding is very similar to OHE and both of them are commonly used in Linear Regression (LR) types of models.\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# this method didn't work because of RAM memory. \n",
    "# SE_encoder =SumEncoder(feature_list)\n",
    "# train_se = SE_encoder.fit_transform(train[feature_list], target)\n",
    "# test_se = SE_encoder.transform(test[feature_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helmert Encoder\n",
    "\n",
    "**Helmert Encoding** is a third commonly used type of categorical encoding for regression along with OHE and Sum Encoding. \n",
    "\n",
    "It compares each level of a categorical variable to the mean of the subsequent levels. \n",
    "\n",
    "This type of encoding can be useful in certain situations where levels of the categorical variable are ordered. (not this dataset)\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# this method didn't work because of RAM memory. \n",
    "# HE_encoder = HelmertEncoder(feature_list)\n",
    "# train_he = HE_encoder.fit_transform(train[feature_list], target)\n",
    "# test_he = HE_encoder.transform(test[feature_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Frequency Encoder\n",
    "\n",
    "This method encodes by frequency.\n",
    "\n",
    "Create a new feature with the number of categories from the training data.\n",
    "\n",
    "I will not proceed separately in this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Target Encoder\n",
    "\n",
    "This is a work in progress for many kernels.\n",
    "\n",
    "The encoded category values are calculated according to the following formulas:\n",
    "\n",
    "$$s = \\frac{1}{1+exp(-\\frac{n-mdl}{a})}$$\n",
    "\n",
    "$$\\hat{x}^k = prior * (1-s) + s * \\frac{n^{+}}{n}$$\n",
    "\n",
    "- mdl means **'min data in leaf'**\n",
    "- a means **'smooth parameter, power of regularization'**\n",
    "\n",
    "Target Encoder is a powerful, but it has a huuuuuge disadvantage \n",
    "\n",
    "> **target leakage**: it uses information about the target. \n",
    "\n",
    "To reduce the effect of target leakage, \n",
    "\n",
    "- Increase regularization\n",
    "- Add random noise to the representation of the category in train dataset (some sort of augmentation)\n",
    "- Use Double Validation (using other validation)\n",
    "\n",
    "Let's use while being careful about overfitting.\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.2 s, sys: 1.96 s, total: 14.2 s\n",
      "Wall time: 10.6 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin_0</th>\n",
       "      <th>bin_1</th>\n",
       "      <th>bin_2</th>\n",
       "      <th>bin_3</th>\n",
       "      <th>bin_4</th>\n",
       "      <th>nom_0</th>\n",
       "      <th>nom_1</th>\n",
       "      <th>nom_2</th>\n",
       "      <th>nom_3</th>\n",
       "      <th>nom_4</th>\n",
       "      <th>...</th>\n",
       "      <th>nom_8</th>\n",
       "      <th>nom_9</th>\n",
       "      <th>ord_0</th>\n",
       "      <th>ord_1</th>\n",
       "      <th>ord_2</th>\n",
       "      <th>ord_3</th>\n",
       "      <th>ord_4</th>\n",
       "      <th>ord_5</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.302537</td>\n",
       "      <td>0.290107</td>\n",
       "      <td>0.327145</td>\n",
       "      <td>0.360978</td>\n",
       "      <td>0.307162</td>\n",
       "      <td>0.242813</td>\n",
       "      <td>0.237743</td>\n",
       "      <td>...</td>\n",
       "      <td>0.372694</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>2</td>\n",
       "      <td>0.403885</td>\n",
       "      <td>0.257877</td>\n",
       "      <td>0.306993</td>\n",
       "      <td>0.208354</td>\n",
       "      <td>0.401186</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.302537</td>\n",
       "      <td>0.290107</td>\n",
       "      <td>0.327145</td>\n",
       "      <td>0.290054</td>\n",
       "      <td>0.359209</td>\n",
       "      <td>0.289954</td>\n",
       "      <td>0.304164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.076924</td>\n",
       "      <td>1</td>\n",
       "      <td>0.403885</td>\n",
       "      <td>0.326315</td>\n",
       "      <td>0.206599</td>\n",
       "      <td>0.186877</td>\n",
       "      <td>0.303880</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.309384</td>\n",
       "      <td>0.290107</td>\n",
       "      <td>0.241790</td>\n",
       "      <td>0.290054</td>\n",
       "      <td>0.293085</td>\n",
       "      <td>0.289954</td>\n",
       "      <td>0.353951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223022</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>1</td>\n",
       "      <td>0.317175</td>\n",
       "      <td>0.403126</td>\n",
       "      <td>0.306993</td>\n",
       "      <td>0.351864</td>\n",
       "      <td>0.206843</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.309384</td>\n",
       "      <td>0.290107</td>\n",
       "      <td>0.351052</td>\n",
       "      <td>0.290054</td>\n",
       "      <td>0.307162</td>\n",
       "      <td>0.339793</td>\n",
       "      <td>0.329472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.325123</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>1</td>\n",
       "      <td>0.403885</td>\n",
       "      <td>0.360961</td>\n",
       "      <td>0.330148</td>\n",
       "      <td>0.208354</td>\n",
       "      <td>0.355985</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.309384</td>\n",
       "      <td>0.333773</td>\n",
       "      <td>0.351052</td>\n",
       "      <td>0.290054</td>\n",
       "      <td>0.293085</td>\n",
       "      <td>0.339793</td>\n",
       "      <td>0.329472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.376812</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.403885</td>\n",
       "      <td>0.225214</td>\n",
       "      <td>0.206599</td>\n",
       "      <td>0.351864</td>\n",
       "      <td>0.404345</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bin_0  bin_1  bin_2     bin_3     bin_4     nom_0     nom_1     nom_2  \\\n",
       "0      0      0      0  0.302537  0.290107  0.327145  0.360978  0.307162   \n",
       "1      0      1      0  0.302537  0.290107  0.327145  0.290054  0.359209   \n",
       "2      0      0      0  0.309384  0.290107  0.241790  0.290054  0.293085   \n",
       "3      0      1      0  0.309384  0.290107  0.351052  0.290054  0.307162   \n",
       "4      0      0      0  0.309384  0.333773  0.351052  0.290054  0.293085   \n",
       "\n",
       "      nom_3     nom_4  ...     nom_8     nom_9  ord_0     ord_1     ord_2  \\\n",
       "0  0.242813  0.237743  ...  0.372694  0.368421      2  0.403885  0.257877   \n",
       "1  0.289954  0.304164  ...  0.189189  0.076924      1  0.403885  0.326315   \n",
       "2  0.289954  0.353951  ...  0.223022  0.172414      1  0.317175  0.403126   \n",
       "3  0.339793  0.329472  ...  0.325123  0.227273      1  0.403885  0.360961   \n",
       "4  0.339793  0.329472  ...  0.376812  0.200000      1  0.403885  0.225214   \n",
       "\n",
       "      ord_3     ord_4     ord_5  day  month  \n",
       "0  0.306993  0.208354  0.401186    2      2  \n",
       "1  0.206599  0.186877  0.303880    7      8  \n",
       "2  0.306993  0.351864  0.206843    7      2  \n",
       "3  0.330148  0.208354  0.355985    2      1  \n",
       "4  0.206599  0.351864  0.404345    7      8  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TE_encoder = TargetEncoder()\n",
    "train_te = TE_encoder.fit_transform(train[feature_list], target)\n",
    "test_te = TE_encoder.transform(test[feature_list])\n",
    "\n",
    "train_te.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. M-Estimate Encoder\n",
    "\n",
    "**M-Estimate Encoder** is a **simplified version of Target Encoder**. It has only one hyperparameter (Wrong Fomular but did good work?!)\n",
    "\n",
    "$$\\hat{x}^k = \\frac{n^+ + prior * m}{y^+ + m}$$\n",
    "\n",
    "The higher value of m results into stronger shrinking. Recommended values for m is in the range of 1 to 100.\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.6 s, sys: 1.08 s, total: 12.6 s\n",
      "Wall time: 9.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "MEE_encoder = MEstimateEncoder()\n",
    "train_mee = MEE_encoder.fit_transform(train[feature_list], target)\n",
    "test_mee = MEE_encoder.transform(test[feature_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UPDATED : error founded in libarary https://github.com/scikit-learn-contrib/category_encoders/issues/200\n",
    "\n",
    "$$\\hat{x}^k = \\frac{n^+ + prior * m}{n+ + m}$$\n",
    "\n",
    "Thanks to [@ansh422](https://www.kaggle.com/ansh422)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Weight of Evidence Encoder \n",
    "\n",
    "**Weight Of Evidence** is a commonly used target-based encoder in credit scoring. \n",
    "\n",
    "It is a measure of the “strength” of a grouping for separating good and bad risk (default). \n",
    "\n",
    "It is calculated from the basic odds ratio:\n",
    "\n",
    "``` python\n",
    "a = Distribution of Good Credit Outcomes\n",
    "b = Distribution of Bad Credit Outcomes\n",
    "WoE = ln(a / b)\n",
    "```\n",
    "\n",
    "However, if we use formulas as is, it might lead to **target leakage**(and overfit).\n",
    "\n",
    "To avoid that, regularization parameter a is induced and WoE is calculated in the following way:\n",
    "\n",
    "$$nomiinator = \\frac{n^+ + a}{y^+ + 2*a}$$\n",
    "\n",
    "$$denominator = ln(\\frac{nominator}{denominator})$$\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 s, sys: 1.25 s, total: 13.1 s\n",
      "Wall time: 10.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "WOE_encoder = WOEEncoder()\n",
    "train_woe = WOE_encoder.fit_transform(train[feature_list], target)\n",
    "test_woe = WOE_encoder.transform(test[feature_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. James-Stein Encoder\n",
    "\n",
    "**James-Stein Encoder** is a target-based encoder.\n",
    "\n",
    "The idea behind James-Stein Encoder is simple. Estimation of the mean target for category k could be calculated according to the following formula:\n",
    "\n",
    "$$\\hat{x}^k = (1-B) * \\frac{n^+}{n} + B * \\frac{y^+}{y} $$\n",
    "\n",
    "One way to select B is to tune it like a hyperparameter via cross-validation, but Charles Stein came up with another solution to the problem:\n",
    "\n",
    "$$B = \\frac{Var[y^k]}{Var[y^k] + Var[y]}$$\n",
    "\n",
    "Seems quite fair, but James-Stein Estimator has a big disadvantage — it is defined only for normal distribution (which is not the case for any classification task). \n",
    "\n",
    "To avoid that, we can either convert binary targets with a log-odds ratio as it was done in WoE Encoder (which is used by default because it is simple) or use beta distribution.\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.5 s, sys: 1.03 s, total: 12.5 s\n",
      "Wall time: 9.89 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "JSE_encoder = JamesSteinEncoder()\n",
    "train_jse = JSE_encoder.fit_transform(train[feature_list], target)\n",
    "test_jse = JSE_encoder.transform(test[feature_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Leave-one-out Encoder (LOO or LOOE)\n",
    "\n",
    "**Leave-one-out Encoding** is another example of target-based encoders.\n",
    "\n",
    "This encoder calculate mean target of category k for observation j if observation j is removed from the dataset:\n",
    "\n",
    "$$\\hat{x}^k_i = \\frac{\\sum_{j \\neq i}(y_j * (x_j == k) ) - y_i }{\\sum_{j \\neq i} x_j == k}$$\n",
    "\n",
    "While encoding the test dataset, a category is replaced with the mean target of the category k in the train dataset:\n",
    "\n",
    "$$\\hat{x}^k = \\frac{\\sum y_j * (x_j == k)  }{\\sum x_j == k}$$\n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 s, sys: 837 ms, total: 13.6 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "LOOE_encoder = LeaveOneOutEncoder()\n",
    "train_looe = LOOE_encoder.fit_transform(train[feature_list], target)\n",
    "test_looe = LOOE_encoder.transform(test[feature_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Catboost Encoder\n",
    "\n",
    "**Catboost** is a recently created target-based categorical encoder. \n",
    "\n",
    "It is intended to overcome target leakage problems inherent in LOO. \n",
    "\n",
    "If you use `Category-Encoders` it will look like this code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.8 s, sys: 1.33 s, total: 20.1 s\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "CBE_encoder = CatBoostEncoder()\n",
    "train_cbe = CBE_encoder.fit_transform(train[feature_list], target)\n",
    "test_cbe = CBE_encoder.transform(test[feature_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Validation proceeds with single lr and lr with cv.\n",
    "\n",
    "- I will add OneHotEncoder, etc later.\n",
    "- More Fold get better score (my experience)\n",
    "- you can try another solver and another parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test OrdinalEncoder :  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:  0.6058952245541614\n",
      "Test WOEEncoder :  score:  0.7814115175120097\n",
      "Test TargetEncoder :  score:  0.7790232741835907\n",
      "Test MEstimateEncoder :  score:  0.7792025608680453\n",
      "Test JamesSteinEncoder :  score:  0.7719623937439537\n",
      "Test LeaveOneOutEncoder :  score:  0.79576549204064\n",
      "Test CatBoostEncoder :  score:  0.7917094405644212\n",
      "CPU times: user 5min 8s, sys: 2.29 s, total: 5min 10s\n",
      "Wall time: 3min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "encoder_list = [ OrdinalEncoder(), WOEEncoder(), TargetEncoder(), MEstimateEncoder(), JamesSteinEncoder(), LeaveOneOutEncoder() ,CatBoostEncoder()]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train, target, test_size=0.2, random_state=97)\n",
    "\n",
    "for encoder in encoder_list:\n",
    "    print(\"Test {} : \".format(str(encoder).split('(')[0]), end=\" \")\n",
    "    train_enc = encoder.fit_transform(X_train[feature_list], y_train)\n",
    "    #test_enc = encoder.transform(test[feature_list])\n",
    "    val_enc = encoder.transform(X_val[feature_list])\n",
    "    lr = LogisticRegression(C=0.1, solver=\"lbfgs\", max_iter=1000)\n",
    "    lr.fit(train_enc, y_train)\n",
    "    lr_pred = lr.predict_proba(val_enc)[:, 1]\n",
    "    score = auc(y_val, lr_pred)\n",
    "    print(\"score: \", score)\n",
    "    del train_enc\n",
    "    del val_enc\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR with CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 µs, sys: 0 ns, total: 20 µs\n",
      "Wall time: 25.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# CV function original : @Peter Hurford : Why Not Logistic Regression? https://www.kaggle.com/peterhurford/why-not-logistic-regression\n",
    "\n",
    "def run_cv_model(train, test, target, model_fn, params={}, label='model'):\n",
    "    kf = KFold(n_splits=5)\n",
    "    fold_splits = kf.split(train, target)\n",
    "\n",
    "    cv_scores = []\n",
    "    pred_full_test = 0\n",
    "    pred_train = np.zeros((train.shape[0]))\n",
    "    i = 1\n",
    "    for dev_index, val_index in fold_splits:\n",
    "        print('Started {} fold {}/5'.format(label, i))\n",
    "        dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n",
    "        dev_y, val_y = target[dev_index], target[val_index]\n",
    "        pred_val_y, pred_test_y = model_fn(dev_X, dev_y, val_X, val_y, test, params)\n",
    "        pred_full_test = pred_full_test + pred_test_y\n",
    "        pred_train[val_index] = pred_val_y\n",
    "        cv_score = auc(val_y, pred_val_y)\n",
    "        cv_scores.append(cv_score)\n",
    "        print(label + ' cv score {}: {}'.format(i, cv_score))\n",
    "        i += 1\n",
    "        \n",
    "    print('{} cv scores : {}'.format(label, cv_scores))\n",
    "    print('{} cv mean score : {}'.format(label, np.mean(cv_scores)))\n",
    "    print('{} cv std score : {}'.format(label, np.std(cv_scores)))\n",
    "    pred_full_test = pred_full_test / 5.0\n",
    "    results = {'label': label, 'train': pred_train, 'test': pred_full_test, 'cv': cv_scores}\n",
    "    return results\n",
    "\n",
    "\n",
    "def runLR(train_X, train_y, test_X, test_y, test_X2, params):\n",
    "    model = LogisticRegression(**params)\n",
    "    model.fit(train_X, train_y)\n",
    "    pred_test_y = model.predict_proba(test_X)[:, 1]\n",
    "    pred_test_y2 = model.predict_proba(test_X2)[:, 1]\n",
    "    return pred_test_y, pred_test_y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "\n",
    "    lr_params = {'solver': 'lbfgs', 'C': 0.1}\n",
    "\n",
    "    results = list()\n",
    "\n",
    "    for encoder in  [ OrdinalEncoder(), WOEEncoder(), TargetEncoder(), MEstimateEncoder(), JamesSteinEncoder(), LeaveOneOutEncoder() ,CatBoostEncoder()]:\n",
    "        train_enc = encoder.fit_transform(train[feature_list], target)\n",
    "        test_enc = encoder.transform(test[feature_list])\n",
    "        result = run_cv_model(train_enc, test_enc, target, runLR, lr_params, str(encoder).split('(')[0])\n",
    "        results.append(result)\n",
    "    results = pd.DataFrame(results)\n",
    "    results['cv_mean'] = results['cv'].apply(lambda l : np.mean(l))\n",
    "    results['cv_std'] = results['cv'].apply(lambda l : np.std(l))\n",
    "    results[['label','cv_mean','cv_std']].head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even CVs did not solve the target based encoder's overfit problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    for idx, label in enumerate(results['label']):\n",
    "        sub_df = pd.DataFrame({'id': test_id, 'target' : results.iloc[idx]['test']})\n",
    "        sub_df.to_csv(\"LR_{}.csv\".format(label), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
