{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine_translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGfqEVVmiAAk"
      },
      "source": [
        "# Машинный перевод\n",
        "\n",
        "**Частично автоматизированный перевод**\n",
        "\n",
        "Программа помогает переводить части предложений, которые может, но полный перевод делает человек. Например, в специализированных областях, где есть достаточные объемы перевода и устоявшаяся терминология или шаблоны, в компании-переводчике может накопиться база уже готовых выверенных переводов и оттуда фразы можно брать и подставлять автоматически. Это упрощает перевод, передавая рутину системе, а сложные и интересные задачи решает человек.\n",
        "\n",
        "CAT-системы (Computer-aided translation, machine-assisted translation)\n",
        "\n",
        "[Можно еще почитать тут](https://sysblok.ru/nlp/cat-sistemy-i-budushhee-perevoda/)\n",
        "\n",
        "**Полностью машинный перевод**\n",
        "\n",
        "**Статистический машинный перевод**\n",
        "\n",
        "Наиболее частый - PBMT - phrase-based machine translation. Есть пословный, синтаксический и другие. В парах предложений в большом корпусе статистически определяются соответствия кусочков предложений (фраз) и эта информация используется для перевода нового текста. Часто используется как базовая модель, если нет достаточного корпуса для нейронного.\n",
        "\n",
        "**Rule-based**\n",
        "\n",
        "Перевод, основанный на словарях и правилах трансформации предложений при переводе. Есть словарные соответствия и схемы, по которым перестраивается синтаксис. [Например, Apertium](https://www.apertium.org/)\n",
        "\n",
        "\n",
        "**NMT - neural machine translation**\n",
        "\n",
        "Обучается нейронная сеть на большом корпусе, которая учится предсказывать наилучший перевод.\n",
        "\n",
        "\n",
        "**Как оценивается качество?**\n",
        "\n",
        "Например, с помощью BLEU (bilingual evaluation understudy) - доля слов или чаще нграмм, которые совпали в варианте перевода и ответами (0 - все плохо, 1 - отлично, но так не бывает даже у человека). Считается, что 4-граммы наиболее хорошо описывают хорошоую человеческую речь. \n",
        "\n",
        "\n",
        "Крупные переводчики (Яндекс, Гугл) - PMBT и NMT (по возможности).\n",
        "\n",
        "\n",
        "Для обучения моделей нужны параллельные корпуса с выравниванием по предложению, например, [OPUS](https://opus.nlpl.eu/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0UoqbO2nktq"
      },
      "source": [
        "# Выравнивание по словам\n",
        "\n",
        "Можно научиться с помощью статистических алгоритмов определять соответствия слов (или фраз) в языке.\n",
        "\n",
        "\n",
        "Будет использовать программу fast_align"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNY9uy2RshV7",
        "outputId": "de00aeac-c482-4a0f-9cbd-2716a74b018c"
      },
      "source": [
        "! git clone https://github.com/clab/fast_align.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fast_align'...\n",
            "remote: Enumerating objects: 213, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 213 (delta 2), reused 4 (delta 2), pack-reused 204\u001b[K\n",
            "Receiving objects: 100% (213/213), 70.68 KiB | 5.44 MiB/s, done.\n",
            "Resolving deltas: 100% (110/110), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1kdbYGwAkLQ"
      },
      "source": [
        "Собираем ее из исходных файлов"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsD5sXz4qXcl",
        "outputId": "fb3b9d2c-5180-4223-e279-c53be5955d50"
      },
      "source": [
        "! mkdir /content/fast_align/build # создаем папку для сборки\n",
        "%cd fast_align/build # переходим в папку\n",
        "# ! pwd\n",
        "! cmake .. # собираем\n",
        "! make # собираем\n",
        "%cd /content/ # возвращаемся в домашнюю папку\n",
        "# ! pwd\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaMi1B7nAnYy"
      },
      "source": [
        "Скачиваем с OPUS небольшой корпус книг"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4V-cYVAumcq",
        "outputId": "b077c459-e6e1-419a-b1d4-91c4521c70a8"
      },
      "source": [
        "! wget https://object.pouta.csc.fi/OPUS-Books/v1/moses/en-ru.txt.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-17 16:45:56--  https://object.pouta.csc.fi/OPUS-Books/v1/moses/en-ru.txt.zip\n",
            "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.19\n",
            "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1613419 (1.5M) [application/zip]\n",
            "Saving to: ‘en-ru.txt.zip’\n",
            "\n",
            "en-ru.txt.zip       100%[===================>]   1.54M  2.26MB/s    in 0.7s    \n",
            "\n",
            "2021-06-17 16:45:57 (2.26 MB/s) - ‘en-ru.txt.zip’ saved [1613419/1613419]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xcTG3YjAs4i"
      },
      "source": [
        "Распаковываем архив"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSTzNi9jumh8",
        "outputId": "0a84f2e8-973e-4b96-94f3-6935407da370"
      },
      "source": [
        "! unzip en-ru.txt.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  en-ru.txt.zip\n",
            "  inflating: Books.en-ru.en          \n",
            "  inflating: Books.en-ru.ru          \n",
            "  inflating: Books.en-ru.ids         \n",
            "  inflating: README                  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8VAzmNVAu9T"
      },
      "source": [
        "Преобразуем данные в тот вид, в котором они принимаются программой"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HcP3JOu5vW3d"
      },
      "source": [
        "! paste Books.en-ru.en Books.en-ru.ru -d \"\\t\" | sed 's/\\t/ ||| /' > Books.en-ru"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cv7UUNkAzwq"
      },
      "source": [
        "Вот так"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jKwtgi2oxt9Q",
        "outputId": "41f596dc-cfac-499f-fd5d-aacfe8b4abd8"
      },
      "source": [
        "! head Books.en-ru"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Anna Karenina ||| Анна Каренина\n",
            "Leo Tolstoy ||| Толстой Лев Николаевич\n",
            "Vengeance is mine; I will repay. ||| Мне отмщение, и аз воздам\n",
            "VOLUME ONE PART I ||| ЧАСТЬ ПЕРВАЯ\n",
            "CHAPTER I ||| I\n",
            "ALL HAPPY FAMILIES resemble one another, but each unhappy family is unhappy in its own way. ||| Все счастливые семьи похожи друг на друга, каждая несчастливая семья несчастлива по-своему.\n",
            "Everything was upset in the Oblonskys' house. ||| Все смешалось в доме Облонских.\n",
            "The wife had discovered an intrigue between her husband and their former French governess, and declared that she would not continue to live under the same roof with him. ||| Жена узнала, что муж был в связи с бывшею в их доме француженкою-гувернанткой, и объявила мужу, что не может жить с ним в одном доме.\n",
            "This state of things had now lasted for three days, and not only the husband and wife but the rest of the family and the whole household suffered from it. ||| Положение это продолжалось уже третий день и мучительно чувствовалось и самими супругами, и всеми членами семьи, и домочадцами.\n",
            "They all felt that there was no sense in their living together, and that any group of people who had met together by chance at an inn would have had more in common than they. ||| Все члены семьи и домочадцы чувствовали, что нет смысла в их сожительстве и что на каждом постоялом дворе случайно сошедшиеся люди более связаны между собой, чем они, члены семьи и домочадцы Облонских.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twBKWET7A045"
      },
      "source": [
        "Копируем программу в рабочую директорию, чтобы не прописывать длинный путь"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynjsleAMyDIH",
        "outputId": "6f8f1fb0-d54a-4bc8-c5ff-2fe06f3f716c"
      },
      "source": [
        "! mv ./fast_align ./fast_align_git # переименуем папку, а то она будет с таким же именем"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot stat './fast_align': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xglTEXrQx1Qf"
      },
      "source": [
        "! cp ./fast_align_git/build/fast_align ./fast_align\n",
        "! cp ./fast_align_git/build/atools ./atools"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFpjsspRA5-P"
      },
      "source": [
        "**Запускаем**\n",
        "\n",
        "Выравнивание происходит в одну сторону, в другую и потом объединяется информация."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60nm34U7xzQO",
        "outputId": "0a0eb1a6-b19d-420a-da7f-319c60289214"
      },
      "source": [
        "! ./fast_align -i Books.en-ru -d -o -v > forward.align\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ARG=i\n",
            "ARG=d\n",
            "ARG=o\n",
            "ARG=v\n",
            "INITIAL PASS \n",
            ".................\n",
            "expected target length = source length * 0.86155\n",
            "ITERATION 1\n",
            ".................\n",
            "  log_e likelihood: -5.8083e+06\n",
            "  log_2 likelihood: -8.3796e+06\n",
            "     cross entropy: 29.8974\n",
            "        perplexity: 1e+09\n",
            "      posterior p0: 0.08\n",
            " posterior al-feat: -0.174998\n",
            "       size counts: 1614\n",
            "ITERATION 2\n",
            ".................\n",
            "  log_e likelihood: -2.28139e+06\n",
            "  log_2 likelihood: -3.29135e+06\n",
            "     cross entropy: 11.7431\n",
            "        perplexity: 3427.89\n",
            "      posterior p0: 0.0906316\n",
            " posterior al-feat: -0.117567\n",
            "       size counts: 1614\n",
            "  1  model al-feat: -0.209743 (tension=4)\n",
            "  2  model al-feat: -0.164009 (tension=5.84352)\n",
            "  3  model al-feat: -0.147483 (tension=6.77236)\n",
            "  4  model al-feat: -0.138633 (tension=7.37068)\n",
            "  5  model al-feat: -0.133147 (tension=7.79199)\n",
            "  6  model al-feat: -0.129461 (tension=8.10358)\n",
            "  7  model al-feat: -0.126852 (tension=8.34146)\n",
            "  8  model al-feat: -0.124937 (tension=8.52716)\n",
            "     final tension: 8.67456\n",
            "ITERATION 3\n",
            ".................\n",
            "  log_e likelihood: -1.44854e+06\n",
            "  log_2 likelihood: -2.08979e+06\n",
            "     cross entropy: 7.45612\n",
            "        perplexity: 175.597\n",
            "      posterior p0: 0.0551809\n",
            " posterior al-feat: -0.0918306\n",
            "       size counts: 1614\n",
            "  1  model al-feat: -0.123491 (tension=8.67456)\n",
            "  2  model al-feat: -0.118038 (tension=9.30778)\n",
            "  3  model al-feat: -0.114475 (tension=9.83192)\n",
            "  4  model al-feat: -0.112143 (tension=10.2848)\n",
            "  5  model al-feat: -0.110706 (tension=10.6911)\n",
            "  6  model al-feat: -0.11 (tension=11.0686)\n",
            "  7  model al-feat: -0.109978 (tension=11.432)\n",
            "  8  model al-feat: -0.110707 (tension=11.7949)\n",
            "     final tension: 12.1724\n",
            "ITERATION 4\n",
            ".................\n",
            "  log_e likelihood: -1.34052e+06\n",
            "  log_2 likelihood: -1.93396e+06\n",
            "     cross entropy: 6.90011\n",
            "        perplexity: 119.438\n",
            "      posterior p0: 0.0527672\n",
            " posterior al-feat: -0.0847931\n",
            "       size counts: 1614\n",
            "  1  model al-feat: -0.11241 (tension=12.1724)\n",
            "  2  model al-feat: -0.117056 (tension=12.7248)\n",
            "  3  model al-feat: -0.126797 (tension=13.37)\n",
            "  4  model al-feat: -0.142648 (tension=14)\n",
            "  5  model al-feat: -0.142648 (tension=14)\n",
            "  6  model al-feat: -0.142648 (tension=14)\n",
            "  7  model al-feat: -0.142648 (tension=14)\n",
            "  8  model al-feat: -0.142648 (tension=14)\n",
            "     final tension: 14\n",
            "ITERATION 5 (FINAL)\n",
            ".................\n",
            "  log_e likelihood: -1.31688e+06\n",
            "  log_2 likelihood: -1.89986e+06\n",
            "     cross entropy: 6.77846\n",
            "        perplexity: 109.779\n",
            "      posterior p0: 0\n",
            " posterior al-feat: 0\n",
            "       size counts: 1614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqOPNM4YyPo8",
        "outputId": "32486419-8a03-477d-f606-943cc1bd0b18"
      },
      "source": [
        "! ./fast_align -i Books.en-ru -d -o -v -r > reverse.align"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ARG=i\n",
            "ARG=d\n",
            "ARG=o\n",
            "ARG=v\n",
            "ARG=r\n",
            "INITIAL PASS \n",
            ".................\n",
            "expected target length = source length * 1.26205\n",
            "ITERATION 1\n",
            ".................\n",
            "  log_e likelihood: -7.05339e+06\n",
            "  log_2 likelihood: -1.01759e+07\n",
            "     cross entropy: 29.8974\n",
            "        perplexity: 1e+09\n",
            "      posterior p0: 0.08\n",
            " posterior al-feat: -0.172934\n",
            "       size counts: 1614\n",
            "ITERATION 2\n",
            ".................\n",
            "  log_e likelihood: -2.29682e+06\n",
            "  log_2 likelihood: -3.31361e+06\n",
            "     cross entropy: 9.73556\n",
            "        perplexity: 852.503\n",
            "      posterior p0: 0.104449\n",
            " posterior al-feat: -0.121262\n",
            "       size counts: 1614\n",
            "  1  model al-feat: -0.138919 (tension=4)\n",
            "  2  model al-feat: -0.132396 (tension=4.35315)\n",
            "  3  model al-feat: -0.128507 (tension=4.57582)\n",
            "  4  model al-feat: -0.126068 (tension=4.72072)\n",
            "  5  model al-feat: -0.124489 (tension=4.81684)\n",
            "  6  model al-feat: -0.123445 (tension=4.88138)\n",
            "  7  model al-feat: -0.122747 (tension=4.92505)\n",
            "  8  model al-feat: -0.122276 (tension=4.95475)\n",
            "     final tension: 4.97503\n",
            "ITERATION 3\n",
            ".................\n",
            "  log_e likelihood: -1.61138e+06\n",
            "  log_2 likelihood: -2.32473e+06\n",
            "     cross entropy: 6.83017\n",
            "        perplexity: 113.786\n",
            "      posterior p0: 0.0715443\n",
            " posterior al-feat: -0.112448\n",
            "       size counts: 1614\n",
            "  1  model al-feat: -0.121956 (tension=4.97503)\n",
            "  2  model al-feat: -0.119018 (tension=5.16519)\n",
            "  3  model al-feat: -0.117054 (tension=5.29659)\n",
            "  4  model al-feat: -0.115709 (tension=5.38871)\n",
            "  5  model al-feat: -0.114772 (tension=5.45393)\n",
            "  6  model al-feat: -0.114112 (tension=5.5004)\n",
            "  7  model al-feat: -0.113643 (tension=5.53368)\n",
            "  8  model al-feat: -0.113308 (tension=5.55758)\n",
            "     final tension: 5.57479\n",
            "ITERATION 4\n",
            ".................\n",
            "  log_e likelihood: -1.50101e+06\n",
            "  log_2 likelihood: -2.16551e+06\n",
            "     cross entropy: 6.36238\n",
            "        perplexity: 82.2748\n",
            "      posterior p0: 0.0701927\n",
            " posterior al-feat: -0.108257\n",
            "       size counts: 1614\n",
            "  1  model al-feat: -0.113068 (tension=5.57479)\n",
            "  2  model al-feat: -0.111743 (tension=5.67101)\n",
            "  3  model al-feat: -0.110799 (tension=5.74073)\n",
            "  4  model al-feat: -0.11012 (tension=5.79156)\n",
            "  5  model al-feat: -0.109626 (tension=5.82881)\n",
            "  6  model al-feat: -0.109266 (tension=5.85618)\n",
            "  7  model al-feat: -0.109002 (tension=5.87636)\n",
            "  8  model al-feat: -0.108808 (tension=5.89125)\n",
            "     final tension: 5.90226\n",
            "ITERATION 5 (FINAL)\n",
            ".................\n",
            "  log_e likelihood: -1.47175e+06\n",
            "  log_2 likelihood: -2.12328e+06\n",
            "     cross entropy: 6.23832\n",
            "        perplexity: 75.4954\n",
            "      posterior p0: 0\n",
            " posterior al-feat: 0\n",
            "       size counts: 1614\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g926DJTh45Q"
      },
      "source": [
        "! ./atools -i forward.align -j reverse.align -c grow-diag-final-and > result.align"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZLRqtlcBI7V"
      },
      "source": [
        "Получаем такой файл. После этого мы можем соединить текст и эти индексы соответствий и достать необходимую информацию о пословных соответствиях."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUARc6osyoRg",
        "outputId": "0c761a3d-bea4-4a1b-b831-0270811df31e"
      },
      "source": [
        "! head result.align"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0-0 1-1\n",
            "0-0 0-1 1-2\n",
            "0-0 1-1 2-1 2-2 3-3 4-3 5-4\n",
            "0-0 1-0 2-0 2-1 3-1\n",
            "0-0 1-0\n",
            "0-1 1-0 2-1 2-2 3-3 4-3 5-4 7-4 7-5 8-6 9-9 11-7 11-8 14-11 15-11\n",
            "0-0 1-1 2-1 3-2 4-3 5-4 6-4\n",
            "1-0 2-1 2-2 3-2 4-4 4-5 5-5 6-6 7-3 8-3 9-7 10-10 11-8 11-9 13-12 14-11 14-13 15-12 16-16 17-14 18-16 19-17 20-17 21-18 22-19 23-22 24-22 25-23 26-24 27-20 28-21\n",
            "1-0 3-1 4-3 5-3 6-2 7-2 8-4 9-6 10-6 10-7 13-10 14-11 15-8 15-9 16-9 18-10 19-11 20-12 21-14 22-15 23-12 23-13 23-16 24-14 27-17 28-16 29-17\n",
            "0-0 1-0 2-5 3-6 4-7 5-6 6-7 8-9 9-10 10-9 11-11 12-12 13-13 15-9 16-8 17-20 18-14 21-22 22-21 24-15 24-16 25-24 26-29 29-23 30-21 31-28 33-25 34-30 34-31\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZITq5v5_AbZg"
      },
      "source": [
        "**Задание**\n",
        "\n",
        "Достать соответствия и посчитать, какие слова какими переводились.\n",
        "\n",
        "```\n",
        "{\n",
        "    \"семья\": {\n",
        "        \"family\": 4,\n",
        "        \"mother\": 1\n",
        "    }\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEgOUqUd1Ghx"
      },
      "source": [
        "## OpenNMT\n",
        "\n",
        "Нейросетевой перевод. Это небольшой пример, который, как потом станет понятно, не очень удачный. Для нормальной работы необходим больший корпус, больше \"железа\" и дольше обучение. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIvUmVT514b2"
      },
      "source": [
        "! mkdir example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6va8l6uBsz5"
      },
      "source": [
        "Проверяем, что кол-во строк одинаковое"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLKKzqZr1YJ2",
        "outputId": "098be69a-8c5c-480b-90ec-5bd9385a1734"
      },
      "source": [
        "! wc -l Books.en-ru.en"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17496 Books.en-ru.en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwdU25Li2Aig",
        "outputId": "1481bdbd-4220-4807-a78c-a5f675be467f"
      },
      "source": [
        "! wc -l Books.en-ru.ru"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17496 Books.en-ru.ru\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ow6IP1nUBvJ3"
      },
      "source": [
        "Часть данных -  это тренировочная выборка, часть - валидационная, на которой модель проверяет себя по ходу обучения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HrspIgG2G7r"
      },
      "source": [
        "! head -n 14000 Books.en-ru.en > ./example/src-train.txt\n",
        "! head -n 14000 Books.en-ru.ru > ./example/tgt-train.txt\n",
        "! tail -n 3496 Books.en-ru.en > ./example/src-val.txt\n",
        "! tail -n 3496 Books.en-ru.ru > ./example/tgt-val.txt"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X87CA6ptB22N"
      },
      "source": [
        "Создаем словари (какие слова учитываются), 10000 предложений для этого используются. Конфиг там же в папке на гитхабе."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGHDAcVd2jZ-",
        "outputId": "4c488cc5-1469-4d69-e06c-aa4428a70e1f"
      },
      "source": [
        "! onmt_build_vocab -config books_en_ru.yaml -n_sample 10000"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2021-06-17 17:17:54,551 INFO] Counter vocab from 10000 samples.\n",
            "[2021-06-17 17:17:54,551 INFO] Build vocab on 10000 transformed examples/corpus.\n",
            "[2021-06-17 17:17:54,563 INFO] corpus_1's transforms: TransformPipe()\n",
            "[2021-06-17 17:17:54,564 INFO] Loading ParallelCorpus(example/src-train.txt, example/tgt-train.txt, align=None)...\n",
            "[2021-06-17 17:17:54,882 INFO] Counters src:21023\n",
            "[2021-06-17 17:17:54,882 INFO] Counters tgt:35606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Rp8kPJ2B_1k"
      },
      "source": [
        "**Запускаем обучение**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhV1oJYg3ISc",
        "outputId": "2e9cfbe9-41cb-4e1d-a12e-1b1ac33ad19c"
      },
      "source": [
        "! onmt_train -config books_en_ru.yaml"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-06-17 17:17:58,448 INFO] Missing transforms field for corpus_1 data, set to default: [].\n",
            "[2021-06-17 17:17:58,448 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2021-06-17 17:17:58,449 INFO] Missing transforms field for valid data, set to default: [].\n",
            "[2021-06-17 17:17:58,449 INFO] Parsed 2 corpora from -data.\n",
            "[2021-06-17 17:17:58,449 INFO] Get special vocabs from Transforms: {'src': set(), 'tgt': set()}.\n",
            "[2021-06-17 17:17:58,449 INFO] Loading vocab from text file...\n",
            "[2021-06-17 17:17:58,449 INFO] Loading src vocabulary from example/run/example.vocab.src\n",
            "[2021-06-17 17:17:58,485 INFO] Loaded src vocab has 21023 tokens.\n",
            "[2021-06-17 17:17:58,494 INFO] Loading tgt vocabulary from example/run/example.vocab.tgt\n",
            "[2021-06-17 17:17:58,565 INFO] Loaded tgt vocab has 35606 tokens.\n",
            "[2021-06-17 17:17:58,582 INFO] Building fields with vocab in counters...\n",
            "[2021-06-17 17:17:58,651 INFO]  * tgt vocab size: 35610.\n",
            "[2021-06-17 17:17:58,676 INFO]  * src vocab size: 21025.\n",
            "[2021-06-17 17:17:58,677 INFO]  * src vocab size = 21025\n",
            "[2021-06-17 17:17:58,678 INFO]  * tgt vocab size = 35610\n",
            "[2021-06-17 17:17:58,679 INFO] Building model...\n",
            "[2021-06-17 17:18:02,403 INFO] NMTModel(\n",
            "  (encoder): RNNEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(21025, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)\n",
            "  )\n",
            "  (decoder): InputFeedRNNDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(35610, 500, padding_idx=1)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.3, inplace=False)\n",
            "    (rnn): StackedLSTM(\n",
            "      (dropout): Dropout(p=0.3, inplace=False)\n",
            "      (layers): ModuleList(\n",
            "        (0): LSTMCell(1000, 500)\n",
            "        (1): LSTMCell(500, 500)\n",
            "      )\n",
            "    )\n",
            "    (attn): GlobalAttention(\n",
            "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
            "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
            "    )\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=500, out_features=35610, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2021-06-17 17:18:02,403 INFO] encoder: 14520500\n",
            "[2021-06-17 17:18:02,403 INFO] decoder: 41403610\n",
            "[2021-06-17 17:18:02,403 INFO] * number of parameters: 55924110\n",
            "[2021-06-17 17:18:02,404 INFO] Starting training on GPU: [0]\n",
            "[2021-06-17 17:18:02,404 INFO] Start training loop and validate every 500 steps...\n",
            "[2021-06-17 17:18:02,404 INFO] corpus_1's transforms: TransformPipe()\n",
            "[2021-06-17 17:18:02,405 INFO] Loading ParallelCorpus(example/src-train.txt, example/tgt-train.txt, align=None)...\n",
            "[2021-06-17 17:18:09,279 INFO] Step 50/ 1000; acc:   4.69; ppl: 15797259.23; xent: 16.58; lr: 1.00000; 8620/7592 tok/s;      7 sec\n",
            "[2021-06-17 17:18:16,810 INFO] Step 100/ 1000; acc:   3.75; ppl: 77368.13; xent: 11.26; lr: 1.00000; 8750/7590 tok/s;     14 sec\n",
            "[2021-06-17 17:18:24,707 INFO] Step 150/ 1000; acc:   4.25; ppl: 13927.71; xent: 9.54; lr: 1.00000; 8212/7186 tok/s;     22 sec\n",
            "[2021-06-17 17:18:31,149 INFO] Loading ParallelCorpus(example/src-train.txt, example/tgt-train.txt, align=None)...\n",
            "[2021-06-17 17:18:32,052 INFO] Step 200/ 1000; acc:   7.61; ppl: 4041.39; xent: 8.30; lr: 1.00000; 8400/7447 tok/s;     30 sec\n",
            "[2021-06-17 17:18:38,674 INFO] Step 250/ 1000; acc:   7.71; ppl: 2855.27; xent: 7.96; lr: 1.00000; 8873/7799 tok/s;     36 sec\n",
            "[2021-06-17 17:18:46,579 INFO] Step 300/ 1000; acc:   5.57; ppl: 3719.17; xent: 8.22; lr: 1.00000; 8005/6990 tok/s;     44 sec\n",
            "[2021-06-17 17:18:55,108 INFO] Step 350/ 1000; acc:   5.64; ppl: 2937.84; xent: 7.99; lr: 1.00000; 7668/6667 tok/s;     53 sec\n",
            "[2021-06-17 17:19:02,736 INFO] Step 400/ 1000; acc:   7.56; ppl: 1717.88; xent: 7.45; lr: 1.00000; 8238/7253 tok/s;     60 sec\n",
            "[2021-06-17 17:19:05,364 INFO] Loading ParallelCorpus(example/src-train.txt, example/tgt-train.txt, align=None)...\n",
            "[2021-06-17 17:19:10,393 INFO] Step 450/ 1000; acc:   9.60; ppl: 1097.57; xent: 7.00; lr: 1.00000; 8227/7182 tok/s;     68 sec\n",
            "[2021-06-17 17:19:17,021 INFO] Step 500/ 1000; acc:   8.34; ppl: 2021.10; xent: 7.61; lr: 1.00000; 9020/7943 tok/s;     75 sec\n",
            "[2021-06-17 17:19:17,022 INFO] valid's transforms: TransformPipe()\n",
            "[2021-06-17 17:19:17,022 INFO] Loading ParallelCorpus(example/src-val.txt, example/tgt-val.txt, align=None)...\n",
            "[2021-06-17 17:19:23,633 INFO] Validation perplexity: 1190.39\n",
            "[2021-06-17 17:19:23,633 INFO] Validation accuracy: 7.75047\n",
            "[2021-06-17 17:19:23,832 INFO] Saving checkpoint example/run/model_step_500.pt\n",
            "[2021-06-17 17:19:33,040 INFO] Step 550/ 1000; acc:   7.94; ppl: 2247.85; xent: 7.72; lr: 1.00000; 4099/3575 tok/s;     91 sec\n",
            "[2021-06-17 17:19:40,051 INFO] Step 600/ 1000; acc:   9.92; ppl: 1509.45; xent: 7.32; lr: 1.00000; 9280/8131 tok/s;     98 sec\n",
            "[2021-06-17 17:19:46,185 INFO] Loading ParallelCorpus(example/src-train.txt, example/tgt-train.txt, align=None)...\n",
            "[2021-06-17 17:19:47,422 INFO] Step 650/ 1000; acc:  12.95; ppl: 739.52; xent: 6.61; lr: 1.00000; 8175/7179 tok/s;    105 sec\n",
            "[2021-06-17 17:19:54,214 INFO] Step 700/ 1000; acc:  12.87; ppl: 1120.49; xent: 7.02; lr: 1.00000; 8912/7803 tok/s;    112 sec\n",
            "[2021-06-17 17:20:01,887 INFO] Step 750/ 1000; acc:  11.84; ppl: 1585.70; xent: 7.37; lr: 1.00000; 8154/7147 tok/s;    119 sec\n",
            "[2021-06-17 17:20:10,217 INFO] Step 800/ 1000; acc:  12.43; ppl: 1352.32; xent: 7.21; lr: 1.00000; 8003/6949 tok/s;    128 sec\n",
            "[2021-06-17 17:20:17,437 INFO] Step 850/ 1000; acc:  14.75; ppl: 643.54; xent: 6.47; lr: 1.00000; 8652/7649 tok/s;    135 sec\n",
            "[2021-06-17 17:20:19,595 INFO] Loading ParallelCorpus(example/src-train.txt, example/tgt-train.txt, align=None)...\n",
            "[2021-06-17 17:20:24,887 INFO] Step 900/ 1000; acc:  15.37; ppl: 668.42; xent: 6.50; lr: 1.00000; 8231/7208 tok/s;    142 sec\n",
            "[2021-06-17 17:20:31,579 INFO] Step 950/ 1000; acc:  14.46; ppl: 1066.58; xent: 6.97; lr: 1.00000; 9137/7976 tok/s;    149 sec\n",
            "[2021-06-17 17:20:40,473 INFO] Step 1000/ 1000; acc:  14.24; ppl: 1119.76; xent: 7.02; lr: 1.00000; 7454/6482 tok/s;    158 sec\n",
            "[2021-06-17 17:20:40,474 INFO] Loading ParallelCorpus(example/src-val.txt, example/tgt-val.txt, align=None)...\n",
            "[2021-06-17 17:20:47,183 INFO] Validation perplexity: 603.24\n",
            "[2021-06-17 17:20:47,183 INFO] Validation accuracy: 15.8878\n",
            "[2021-06-17 17:20:47,384 INFO] Saving checkpoint example/run/model_step_1000.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4g9CeL5CDPg"
      },
      "source": [
        "Можем записать нужные предложения в файл и потом его передать модели."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDgvnWRu30c6"
      },
      "source": [
        "! echo \"He waited.\" > src-test.txt"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7QO9QRk3rTn",
        "outputId": "4d461ca1-bc73-4760-8659-87b646fed4cf"
      },
      "source": [
        "! onmt_translate -model example/run/model_step_1000.pt -src src-test.txt -gpu 0 -verbose\n"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-06-17 17:23:05,530 INFO] Translating shard 0.\n",
            "[2021-06-17 17:23:05,552 INFO] \n",
            "SENT 1: ['He', 'waited.']\n",
            "PRED 1: Он видел\n",
            "PRED SCORE: -7.3827\n",
            "\n",
            "[2021-06-17 17:23:05,552 INFO] PRED AVG SCORE: -3.6913, PRED PPL: 40.0982\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}